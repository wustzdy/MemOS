import json
import psycopg2
from psycopg2.extras import Json, execute_batch
import numpy as np
import sys
import os
from datetime import datetime

# PolarDB é…ç½®
POLARDB_CONFIG = {
    "host": "memory.pg.polardb.rds.aliyuncs.com",
    "port": 5432,
    "user": "adimin",
    "password": "Openmem0925",
    # "database": "memtensor_memos",
    "database": "test_zdy",
    # "graph_name": "memtensor_memos_graph"
    "graph_name": "test_zdy_graph"
}


class PolarDBGraph:
    def __init__(self, config):
        self.config = config
        self.connection = psycopg2.connect(
            host=config["host"],
            port=config["port"],
            user=config["user"],
            password=config["password"],
            database=config["database"]
        )
        self.graph_name = config.get("graph_name")
        # è®¾ç½®è‡ªåŠ¨æäº¤ä¸ºFalseï¼Œæ‰‹åŠ¨æ§åˆ¶äº‹åŠ¡
        self.connection.autocommit = False
        print("âœ… PolarDBè¿æ¥æˆåŠŸ")

    def update_graph_id_in_properties(self):
        """æ›´æ–°propertieså­—æ®µï¼Œæ·»åŠ graph_id"""
        print("ğŸ”„ å¼€å§‹æ›´æ–°propertieså­—æ®µï¼Œæ·»åŠ graph_id...")
        start_time = datetime.now()

        try:
            with self.connection.cursor() as cursor:
                # æ‰§è¡ŒUPDATEè¯­å¥ï¼Œå°†graph_idæ·»åŠ åˆ°propertiesä¸­
                update_sql = f"""
                    UPDATE {self.graph_name}."Memory"
                    SET properties = agtype_concat(properties, agtype_build_map('graph_id', id::text))
                """
                cursor.execute(update_sql)
                updated_count = cursor.rowcount

                self.connection.commit()

                elapsed = (datetime.now() - start_time).total_seconds()
                print(f"âœ… æˆåŠŸæ›´æ–° {updated_count} æ¡è®°å½•çš„propertieså­—æ®µï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
                return updated_count

        except Exception as e:
            self.connection.rollback()
            print(f"âŒ æ›´æ–°propertieså­—æ®µå¤±è´¥: {e}")
            return 0

    def batch_add_nodes_optimized(self, nodes, batch_size=1000):
        """ä¼˜åŒ–ç‰ˆæ‰¹é‡æ’å…¥èŠ‚ç‚¹"""
        success_count = 0
        error_count = 0
        total_nodes = len(nodes)

        print(f"ğŸš€ å¼€å§‹å¤„ç† {total_nodes} æ¡è®°å½•ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        start_time = datetime.now()

        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for batch_start in range(0, total_nodes, batch_size):
            batch_end = min(batch_start + batch_size, total_nodes)
            current_batch = nodes[batch_start:batch_end]

            batch_success = 0
            batch_errors = []

            try:
                with self.connection.cursor() as cursor:

                    # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
                    insert_data_1024 = []
                    # insert_data_768 = []
                    # insert_data_3072 = []
                    insert_data_no_embedding = []

                    for node in current_batch:
                        try:
                            id_ = node["id"]
                            memory_ = node["memory"]
                            metadata = node["metadata"]

                            # get_graph_id_query = f"""
                            #                  SELECT ag_catalog._make_graph_id('{self.graph_name}'::name, 'Memory'::name, %s::text::cstring)
                            #              """
                            # cursor.execute(get_graph_id_query, (id_,))
                            # graph_id = cursor.fetchone()[0]
                            # properties['graph_id'] = str(graph_id)

                            # æå– embedding
                            embedding = None
                            for embedding_key in ["embedding_1024", "embedding_768", "embedding_3072", "embedding"]:
                                if embedding_key in metadata and metadata[embedding_key]:
                                    embedding = metadata[embedding_key]
                                    break

                            if isinstance(embedding, str):
                                try:
                                    embedding = json.loads(embedding)
                                except json.JSONDecodeError:
                                    print(f"âš ï¸ æ— æ³•è§£æembeddingå­—ç¬¦ä¸²: {embedding_key}")
                                    embedding = None
                            # æ¸…ç† properties
                            properties = self.clean_properties(metadata)
                            properties["id"] = id_
                            properties["memory"] = memory_

                            # æ ¹æ®embeddingç»´åº¦åˆ†ç±»
                            field_name = self.detect_embedding_field(embedding)
                            vector_value = self.convert_to_vector(embedding) if field_name else None

                            if field_name == "embedding" and vector_value:
                                insert_data_1024.append((id_, Json(properties), vector_value))
                            # elif field_name == "embedding_768" and vector_value:
                            #     insert_data_768.append((id_, Json(properties), vector_value))
                            # elif field_name == "embedding_3072" and vector_value:
                            #     insert_data_3072.append((id_, Json(properties), vector_value))
                            else:
                                insert_data_no_embedding.append((id_, Json(properties)))

                        except Exception as e:
                            batch_errors.append(f"ID: {node.get('id', 'unknown')} - {e}")

                    # æ‰¹é‡æ’å…¥ä¸åŒç»´åº¦çš„æ•°æ®
                    if insert_data_1024:
                        insert_sql_1024 = f"""
                            INSERT INTO "Memory" (id, properties, embedding)
                            VALUES (ag_catalog._make_graph_id('{self.graph_name}'::name, 'Memory'::name, %s::text::cstring), %s, %s)
                        """
                        execute_batch(cursor, insert_sql_1024, insert_data_1024)
                        batch_success += len(insert_data_1024)

                    # if insert_data_768:
                    #     insert_sql_768 = f"""
                    #         INSERT INTO "Memory" (id, properties, embedding_768)
                    #         VALUES (ag_catalog._make_graph_id('{self.graph_name}'::name, 'Memory'::name, %s::text::cstring), %s, %s)
                    #     """
                    #     execute_batch(cursor, insert_sql_768, insert_data_768)
                    #     batch_success += len(insert_data_768)
                    #
                    # if insert_data_3072:
                    #     insert_sql_3072 = f"""
                    #         INSERT INTO "Memory" (id, properties, embedding_3072)
                    #         VALUES (ag_catalog._make_graph_id('{self.graph_name}'::name, 'Memory'::name, %s::text::cstring), %s, %s)
                    #     """
                    #     execute_batch(cursor, insert_sql_3072, insert_data_3072)
                    #     batch_success += len(insert_data_3072)

                    if insert_data_no_embedding:
                        insert_sql_no_embedding = f"""
                            INSERT INTO "Memory" (id, properties)
                            VALUES (ag_catalog._make_graph_id('{self.graph_name}'::name, 'Memory'::name, %s::text::cstring), %s)
                        """
                        execute_batch(cursor, insert_sql_no_embedding, insert_data_no_embedding)
                        batch_success += len(insert_data_no_embedding)

                # æäº¤å½“å‰æ‰¹æ¬¡
                self.connection.commit()
                success_count += batch_success
                error_count += len(batch_errors)

                # è¿›åº¦æ˜¾ç¤º
                elapsed = (datetime.now() - start_time).total_seconds()
                progress = (batch_end / total_nodes) * 100
                estimated_total = (elapsed / batch_end) * total_nodes if batch_end > 0 else 0
                remaining = estimated_total - elapsed

                print(f"ğŸ“Š è¿›åº¦: {batch_end}/{total_nodes} ({progress:.1f}%) | "
                      f"æˆåŠŸ: {success_count} | å¤±è´¥: {error_count} | "
                      f"å·²ç”¨: {elapsed:.0f}s | å‰©ä½™: {remaining:.0f}s")

                # è¾“å‡ºæ‰¹æ¬¡é”™è¯¯
                if batch_errors:
                    print(f"âŒ æœ¬æ‰¹æ¬¡é”™è¯¯: {len(batch_errors)} æ¡")
                    for i, error in enumerate(batch_errors[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                        print(f"   {i + 1}. {error}")
                    if len(batch_errors) > 5:
                        print(f"   ... è¿˜æœ‰ {len(batch_errors) - 5} ä¸ªé”™è¯¯")

            except Exception as e:
                self.connection.rollback()
                error_count += len(current_batch)
                print(f"âŒ æ‰¹æ¬¡ {batch_start}-{batch_end} æ•´ä½“å¤±è´¥: {e}")

        total_time = (datetime.now() - start_time).total_seconds()
        print(f"âœ… æ‰¹é‡æ’å…¥å®Œæˆ: æˆåŠŸ {success_count} æ¡, å¤±è´¥ {error_count} æ¡, æ€»è€—æ—¶: {total_time:.2f}ç§’")

        return success_count, error_count

    def clean_properties(self, props):
        """ç§»é™¤å‘é‡å­—æ®µ"""
        vector_keys = {"embedding", "embedding_1024", "embedding_3072", "embedding_768"}
        if not isinstance(props, dict):
            return {}
        return {k: v for k, v in props.items() if k not in vector_keys}

    def detect_embedding_field(self, embedding_list):
        """æ£€æµ‹ embedding ç»´åº¦å¹¶è¿”å›å¯¹åº”çš„å­—æ®µå"""
        if not embedding_list:
            return None
        dim = len(embedding_list)
        # print("---------",dim)
        if dim == 1024:
            return "embedding"
        elif dim == 768:
            return "embedding_768"
        elif dim == 3072:
            return "embedding_3072"
        else:
            print(f"âš ï¸ æœªçŸ¥ embedding ç»´åº¦ {dim}ï¼Œè·³è¿‡è¯¥å‘é‡")
            return None

    def convert_to_vector(self, embedding_list):
        """å°† embedding åˆ—è¡¨è½¬æ¢ä¸ºå‘é‡å­—ç¬¦ä¸²"""
        if not embedding_list:
            return None
        if isinstance(embedding_list, np.ndarray):
            embedding_list = embedding_list.tolist()
        return "[" + ",".join(str(float(x)) for x in embedding_list) + "]"

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.connection:
            self.connection.close()
            print("ğŸ”’ PolarDBè¿æ¥å·²å…³é—­")


def getPolarDb():
    """ç›´æ¥åˆ›å»º PolarDB å›¾æ•°æ®åº“å®ä¾‹"""
    return PolarDBGraph(POLARDB_CONFIG)


def process_metadata(item):
    """å¤„ç†å…ƒæ•°æ®ï¼Œæå–å’Œè½¬æ¢å­—æ®µ"""
    metadata = {}
    for key, value in item.items():
        if key not in ["id", "memory"]:
            # ç±»å‹è½¬æ¢
            if key == "confidence":
                try:
                    metadata[key] = float(value)
                except (ValueError, TypeError):
                    metadata[key] = value
            elif key == "sources" or key == "usage":
                if isinstance(value, str):
                    try:
                        parsed_value = json.loads(value)
                        metadata[key] = [json.dumps(item) for item in parsed_value] if isinstance(parsed_value,
                                                                                                  list) else [
                            json.dumps(parsed_value)]
                    except json.JSONDecodeError:
                        metadata[key] = value
                else:
                    metadata[key] = value
            elif key == "tags":
                if isinstance(value, str):
                    if value.startswith('[') and value.endswith(']'):
                        try:
                            metadata[key] = json.loads(value)
                        except json.JSONDecodeError:
                            metadata[key] = [tag.strip() for tag in value[1:-1].split(',')]
                    else:
                        metadata[key] = value
                else:
                    metadata[key] = value
            else:
                metadata[key] = value
    return metadata


def extract_embedding(item):
    """ä»æ•°æ®é¡¹ä¸­æå–embedding"""
    embedding = None
    for embedding_key in ["embedding_1024", "embedding_768", "embedding_3072", "embedding"]:
        if embedding_key in item and item[embedding_key]:
            embedding_value = item[embedding_key]
            if isinstance(embedding_value, str):
                try:
                    embedding = json.loads(embedding_value)
                except json.JSONDecodeError:
                    print(f"âš ï¸ æ— æ³•è§£æembeddingå­—ç¬¦ä¸²: {embedding_key}")
                    embedding = None
            else:
                embedding = embedding_value
            break
    return embedding


def prepare_nodes_for_insertion(data_list):
    """å‡†å¤‡è¦æ’å…¥çš„èŠ‚ç‚¹æ•°æ®"""
    nodes_to_insert = []
    processed_count = 0
    skipped_count = 0

    for item in data_list:
        id_ = item.get("id")
        memory_ = item.get("memory")

        if not id_ or not memory_:
            print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ•°æ®: IDæˆ–memoryä¸ºç©º")
            skipped_count += 1
            continue

        # å¤„ç†å…ƒæ•°æ®
        metadata = process_metadata(item)

        # å¤„ç†embeddingå­—æ®µ
        embedding = extract_embedding(item)
        if embedding:
            metadata["embedding"] = embedding

        # æ„å»ºæ’å…¥çš„æ•°æ®
        nodes_to_insert.append({
            "id": id_,
            "memory": memory_,
            "metadata": metadata
        })
        processed_count += 1

        # æ˜¾ç¤ºè¿›åº¦
        if processed_count % 10000 == 0:
            print(f"ğŸ“ å·²é¢„å¤„ç† {processed_count} æ¡æ•°æ®")

    print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: æœ‰æ•ˆ {processed_count} æ¡, è·³è¿‡ {skipped_count} æ¡")
    return nodes_to_insert


def insert_data_optimized(data_list, batch_size=1000):
    """ä¼˜åŒ–ç‰ˆæ•°æ®æ’å…¥"""
    graph = getPolarDb()

    # æ•°æ®é¢„å¤„ç†
    print("ğŸ”„ å¼€å§‹é¢„å¤„ç†æ•°æ®...")
    nodes_to_insert = prepare_nodes_for_insertion(data_list)

    if not nodes_to_insert:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ•°æ®éœ€è¦æ’å…¥")
        graph.close()
        return 0, 0

    # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæ‰¹é‡æ’å…¥
    print("ğŸš€ å¼€å§‹æ‰¹é‡æ’å…¥æ•°æ®...")
    success_count, error_count = graph.batch_add_nodes_optimized(nodes_to_insert, batch_size)

    graph.close()
    return success_count, error_count



def load_data_from_file(filename):
    """ä»æ–‡ä»¶åŠ è½½æ•°æ®"""
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ–‡ä»¶: {filename}")
    try:
        with open(filename, "r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"ğŸ“‚ ä»æ–‡ä»¶ {filename} åŠ è½½äº† {len(data)} æ¡è®°å½•")
        return data
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
        return []

def update_graph():
    print("-----------update_graph[start]")
    graph = getPolarDb()
    graph.update_graph_id_in_properties()
    print("---------update_graph[end]")

def insert_data(conn, data):
    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = datetime.now()


    if not data:
        print("âš ï¸ æ²¡æœ‰æ•°æ®")
        return

    print(f"ğŸ¯ æ€»å…±éœ€è¦å¤„ç† {len(data)} æ¡è®°å½•")
    success_count, error_count = insert_data_optimized(data, batch_size=1000)

    # è®¡ç®—æ€»è€—æ—¶
    total_time = (datetime.now() - total_start_time).total_seconds()
    minutes, seconds = divmod(total_time, 60)
    hours, minutes = divmod(minutes, 60)

    print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   âœ… æˆåŠŸ: {success_count} æ¡")
    print(f"   âŒ å¤±è´¥: {error_count} æ¡")
    print(f"   â±ï¸  æ€»è€—æ—¶: {int(hours)}å°æ—¶{int(minutes)}åˆ†é’Ÿ{seconds:.2f}ç§’")

def main():
    json_file = r"/Users/ccl/Desktop/file/export13/ceshi/ceshi.json"

    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = datetime.now()

    # åŠ è½½æ•°æ®
    data = load_data_from_file(json_file)
    if not data:
        print("âš ï¸ æ²¡æœ‰æ•°æ®")
        return

    print(f"ğŸ¯ æ€»å…±éœ€è¦å¤„ç† {len(data)} æ¡è®°å½•")

    # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼Œè®¾ç½®æ‰¹æ¬¡å¤§å°ä¸º1000
    # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æ‰¹æ¬¡å¤§å°ï¼š
    # - ç½‘ç»œå¥½ï¼š1000-2000
    # - ç½‘ç»œä¸€èˆ¬ï¼š500-1000
    # - å†…å­˜æœ‰é™ï¼š200-500
    success_count, error_count = insert_data_optimized(data, batch_size=1000)

    # è®¡ç®—æ€»è€—æ—¶
    total_time = (datetime.now() - total_start_time).total_seconds()
    minutes, seconds = divmod(total_time, 60)
    hours, minutes = divmod(minutes, 60)

    print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"   âœ… æˆåŠŸ: {success_count} æ¡")
    print(f"   âŒ å¤±è´¥: {error_count} æ¡")
    print(f"   â±ï¸  æ€»è€—æ—¶: {int(hours)}å°æ—¶{int(minutes)}åˆ†é’Ÿ{seconds:.2f}ç§’")

    if success_count > 0:
        records_per_second = success_count / total_time
        print(f"   ğŸš€ å¤„ç†é€Ÿåº¦: {records_per_second:.2f} æ¡/ç§’")


if __name__ == "__main__":
    main()